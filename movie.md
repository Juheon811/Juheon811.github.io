---
layout: inner
title: "Movie Review Text Mining: Sentiment Analysis of Avengers: Endgame"
permalink: /movie/
---
# ğŸ¬ Movie Review Text Mining: Sentiment Analysis of Avengers: Endgame

---
<br><br>
## ğŸ¥ 1. Introduction

*Avengers: Endgame* was one of the most globally successful films of the last decade.  
Both Korean and American audiences rated the movie extremely highly.

- ğŸ‡ºğŸ‡¸ **Rotten Tomatoes (U.S.)**: 8.4 / 10  
- ğŸ‡°ğŸ‡· **Naver Movie (Korea)**: 9.5 / 10  

At first glance, the conclusion seems simple -> Both countries loved the movie.

But hereâ€™s the real question ğŸ‘‡

> If both countries gave similarly high ratings, does a high rating always mean strong positive emotion in text?

This project investigates that question using Natural Language Processing (NLP)

---
<br><br>
## ğŸ¯ 2. Research Objective

The goal of this project is to analyze and compare movie reviews of *Avengers: Endgame* from the United States and Korea using text mining techniques.

More specifically, this study aims to:

1. ğŸ” Compare frequently used words in U.S. and Korean reviews  
2. ğŸ“Š Analyze sentiment distributions in both countries  
3. ğŸ¤” Examine why high ratings may still produce different sentiment patterns  
4. ğŸŒ Explore whether cultural communication styles influence sentiment detection  

Although both countries rated the movie highly, the distribution of textual sentiment may reveal deeper structural differences.

This is not just about whether people liked the movie. 
It is about **how emotion is linguistically encoded across cultures.**

---
<br><br>
## ğŸ“š 3. Theoretical Background

### 3.1 â­ Star Ratings vs Textual Sentiment

Prior research suggests that star ratings and textual sentiment do not always align.

Wan & Nakayama (2022) demonstrated that numerical ratings reflect overall satisfaction, while textual reviews often capture more nuanced emotional expression.

This means:

- Two countries may assign similar ratings  
- But differ in emotional intensity within the text  

So relying solely on rating scores may hide cross-cultural variation in how emotions are expressed.

---

### 3.2 ğŸŒ Cultural Communication Styles

Cultural communication theory provides an additional lens.

According to cross-cultural communication research (e.g., Brand et al., 2022), cultures can be broadly categorized.

In high-context cultures:
- Communication tends to be indirect and subtle  
- Emotional evaluation is often implied rather than explicitly stated  

In low-context cultures:
- Communication is explicit and direct  
- Emotional reactions are often expressed with strong evaluative words  

This distinction is important because sentiment analysis models rely heavily on explicit emotional vocabulary.

If emotion is expressed indirectly, a model may struggle to detect it accurately.

---
<br><br>
## ğŸ“¦ 4. Data Collection ğŸ“¦

To explore this question, I collected movie review data from two major platforms:

- ğŸ‡ºğŸ‡¸ **Rotten Tomatoes**  
- ğŸ‡°ğŸ‡· **Naver Movie**

### Dataset Overview

- ğŸ“„ Total reviews: **600**
- ğŸ‡ºğŸ‡¸ 300 English reviews
- ğŸ‡°ğŸ‡· 300 Korean reviews
- ğŸ“ Text-only reviews (free-text format)
- Each dataset contains a single column: `review`

All reviews were manually collected from publicly available sources and stored in CSV format.

---
<br><br>

## ğŸŸ¥ USA


### ğŸ”¹Data Processing


#### 1. Preprocess Text Data 

```python
def clean_text(text):
    text = str(text)                        # make sure it is a string
    text = re.sub(r'\n', ' ', text)         # remove new lines
    text = re.sub(r'\r', '', text)          # remove return marks
    text = re.sub(r'\t', ' ', text)         # remove tabs
    text = re.sub(r'\.', '', text)          # remove dots
    text = re.sub(r"[^A-Za-z\s]", "", text) # remove special marks, keep letters
    text = re.sub(r'\d+', '', text)         # remove numbers
    text = re.sub(r'\s+', ' ', text)        # fix many spaces to one
    text = text.strip().lower()             # cut side spaces and lowercase
```


#### 2. Remove Stopword and Domain-Specific Word

```python
stop_words = set(stopwords.words("english"))

keep_words = {"not", "no", "nor", "but", "however", "though", "although", "without"}
stop_words = stop_words - keep_words

domain_stopwords = {
    "movie", "film", "movies", "films",
    "cinema", "endgame", "marvel",
    "avengers", "mcu", "infinity"
}

def remove_stopwords(text):
    tokens = [
        w for w in text.split()
        if w not in stop_words
        and w not in domain_stopwords
        and len(w) > 2
    ]
    return tokens
```


#### 3. Normalization & Lemmatization

```python
lemmatizer = WordNetLemmatizer()

def normalize_and_lemmatize(tokens):

    # normalize films â†’ movie
    tokens = [re.sub(r"\bfilms?\b", "movie", w) for w in tokens]

    # apply lemmatization
    tokens = [lemmatizer.lemmatize(w) for w in tokens]

    return tokens
```

---


### ğŸ”¹Word Frequency Analysis (U.S. Reviews)

The bar chart below presents the top 30 most frequent words in U.S. reviews.

<p align="left">
  <img src="/img/posts/ubar.png" width="650">
</p>

Strong evaluative terms such as *best*, *perfect*, *great*, and *amazing* dominate the ranking. This indicates that American reviewers frequently rely on direct and high-intensity emotional vocabulary.

---


### ğŸ”¹Word Cloud Visualization

To complement the frequency distribution, a word cloud was generated to visualize important words.

<p align="left">
  <img src="/img/posts/ucloud.png" width="650">
</p>

The visual dominance of words like *best*, *time*, *great*, and *perfect* reinforces the observation that positivity is expressed explicitly and emphatically. While the bar chart provides precise rankings, the word cloud highlights the emotional intensity embedded in word choice.

---


### ğŸ”¹ Sentiment Analysis (VADER)

To quantify emotional polarity in U.S. reviews, I applied **VADER (Valence Aware Dictionary and sEntiment Reasoner)** â€” a lexicon-based sentiment analysis model designed for social and review text.

VADER computes a compound sentiment score ranging from -1 (most negative) to +1 (most positive).


#### Compute Sentiment Scores

```python
# Sentiment Analysis (VADER)
sia = SentimentIntensityAnalyzer()

# Calculate sentiment scores (VADER compound score for each cleaned review)
df["sentiment_score"] = df["clean_review"].apply(lambda x: sia.polarity_scores(x)["compound"])

def classify(score):
    if score > 0:
        return "positive"
    elif score < 0:
        return "negative"
    else:
        return "neutral"

# Apply classification to each sentiment score
df["sentiment"] = df["sentiment_score"].apply(classify)
```

Reviews were categorized based on compound score:

Positive â†’ score > 0

Negative â†’ score < 0

Neutral â†’ score = 0

---


### ğŸ”¹ Sentiment Distribution (U.S. Reviews)

<p align="left">
  <img src="/img/posts/uchart.png" width="650">
</p>

The overwhelming dominance of positive reviews indicates that American audiences expressed strong approval of the film. This result aligns closely with the earlier word frequency analysis, where highly evaluative terms such as *best*, *perfect*, *great*, and *amazing* appeared frequently. This shows that American reviewers tend to express their emotions strongly and directly.



## ğŸŸ¦ KOREA


### ğŸ”¹ Data Processing


#### 1. Preprocess Text Data 

```python
def clean_korean_text(text):
    text = str(text)
    text = re.sub(r'\n', ' ', text)       # Remove line breaks
    text = re.sub(r'\r', '', text)        # Remove carriage returns
    text = re.sub(r'\t', ' ', text)       # Remove tab characters
    text = re.sub(r'[~!@#$%^&*()_+=<>?/.,:;\'\"â€â€œâ€˜â€™â€¦â˜…â˜†â™¥â™¡]', '', text)  # Remove special symbols
    text = re.sub(r'\d+', '', text)       # Remove numbers
    text = re.sub(r'[ã„±-ã…ã…-ã…£]+', '', text)  # Remove isolated Korean consonants/vowels (e.g., ã…‹ã…‹, ã…ã…)
    text = re.sub(r'\s+', ' ', text)      # Remove multiple spaces
    text = text.strip()
```

#### 2. Remove Stopword and Domain-Specific Word

```python
stop_words = [
    # Particles, endings, and adverbs (grammatical function words)
    "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ì—ì„œ", "ìœ¼ë¡œ", "ë¡œ", "ì™€", "ê³¼",
    "ë„", "ë§Œ", "ë³´ë‹¤", "ì²˜ëŸ¼", "ê¹Œì§€", "ê»˜ì„œ", "í•œí…Œ", "ì—ê²Œ", "ë¼ê³ ",
    "ê·¸ë¦¬ê³ ", "ê·¸ë˜ì„œ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë˜", "ë˜í•œ", "ê·¼ë°", "ê·¸ëŸ°ë°",
    "ë­”ê°€", "ì¢€", "ë„ˆë¬´", "ì •ë§", "ì§„ì§œ", "ì™„ì „", "ì•„ì£¼", "ë§ì´", "ë˜ê²Œ", "ê·¸ëƒ¥",
    "ì´ê±´", "ì €ê±´", "ê·¸ê±´", "ìš°ë¦¬", "ë‚´ê°€", "ì´ë²ˆ", "ì´ì œ", "ë˜ëŠ”", "ë‹¤ì‹œ",
    "í•˜ë‹¤", "ì´ë‹¤",

    # Interjections and onomatopoeia (do not contribute directly to sentiment)
    "ì™€", "ì•„", "ì–´", "ìŒ", "í—", "ã…‹ã…‹", "ã…ã…", "ã… ", "ã…œ", "ã„·ã„·", "í•˜í•˜", "íœ´", "ìº¬",

    # Miscellaneous unnecessary words
    "ë•Œë¬¸", "ì •ë„", "ê²ƒ", "ê±°", "ê²Œ", "ìˆ˜", "ë“¯", "ìš”", "ì£ ", "ë„¤", "ë°", "ì¤‘", "ì˜í™”", "ë§ˆë¸”", "ì—”ë“œê²Œì„",
    "ê±´ê°€", "ì¸ê°€", "ê±°ë‚˜", "ë¼ë„", "ê±°ë“ ìš”", "ë„¤ìš”", "ì…ë‹ˆë‹¤", "í–ˆìŠµë‹ˆë‹¤", "ë´¤ì–´ìš”","ì–´ë²¤ì ¸ìŠ¤", "ì¬ë¯¸ìˆë‹¤"
]
```

#### 3. Normalization & Tokenization

```python
okt = Okt()

def normalize_and_tokenize(text):

    # tokenization
    tokens = okt.morphs(text, stem=True) # It is a morphological analysis function that splits a sentence into individual word units.

    # stopword removal and length filtering
    tokens = [w for w in tokens if w not in stop_words and len(w) > 1]

    return " ".join(tokens)
```

---

---

### ğŸ”¹ Word Frequency Analysis (Korean Reviews)

The bar chart below presents the top 30 most frequent words in Korean reviews.

<p align="left">
  <img src="/img/posts/kbar.png" width="650">
</p>


Words such as *ì¬ë°Œë‹¤* (fun), *ì¢‹ë‹¤* (good), *ìµœê³ * (the best), and *ê°ë™* (emotion) appear frequently, indicating clear positive sentiment. At the same time, experiential words like *ì‹œê°„* (time), *ë§ˆì§€ë§‰* (last), and *ì‹œë¦¬ì¦ˆ* (series) are also prominent. This suggests that Korean reviewers express emotion clearly, but often within a broader narrative or situational context rather than relying solely on intensifiers.

---

### ğŸ”¹ Word Cloud Visualization

To complement the frequency distribution, a word cloud was generated to visualize important words in Korean reviews.

<p align="left">
  <img src="/img/posts/kcloud.png" width="650">
</p>

The word cloud highlights emotional terms such as *ì¬ë°Œë‹¤*, *ì¢‹ë‹¤*, and *ê°ë™*, but it also emphasizes relational and experiential words like *ë³´ë‹¤* (to watch), *ì‹œë¦¬ì¦ˆ* (series), and *ì‹œê°„* (time). Compared to the U.S. reviews, Korean reviews still contain strong emotional expressions, but these words coexist with contextual and narrative terms. This pattern suggests that emotional expression in Korean reviews is often intertwined with storytelling and experiential reflection rather than expressed purely through intensifiers.

---

### ğŸ”¹ Sentiment Analysis (KNU Sentiment Lexicon)

To quantify emotional polarity in Korean reviews, I applied a **lexicon-based sentiment analysis approach** using the KNU Korean Sentiment Lexicon.

The KNU lexicon was constructed from a large Korean lexical database using deep learning-based classification of dictionary definitions.

Unlike VADER, this method does not generate a normalized compound score. Instead, it computes the overall sentiment score by summing the polarity values of all matched sentiment words within each review.

---

#### Compute Sentiment Scores

```python
# Load the sentiment lexicon
knu_lex = pd.read_csv("knu_sentiment_lexicon.csv")

# Convert lexicon into dictionary
sentiment_dict = dict(zip(knu_lex["word"], knu_lex["polarity"]))

# Function to calculate sentiment score
def get_sentiment_score(text):
    tokens = okt.morphs(text)
    score = 0
    for word in tokens:
        if word in sentiment_dict:
            score += sentiment_dict[word]
    return score

# Apply sentiment scoring
df_kor["sentiment_score"] = df_kor["clean_review"].apply(get_sentiment_score)

def classify(score):
    if score > 0:
        return "positive"
    elif score < 0:
        return "negative"
    else:
        return "neutral"

df_kor["sentiment"] = df_kor["sentiment_score"].apply(classify)
```
